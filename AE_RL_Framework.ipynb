{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch.quantization as quantization\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "cMO3ICQX7X0M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrIndmyK7X_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Simulate ML-KEM Key Generation Traces"
      ],
      "metadata": {
        "id": "7g2-FUkdu1xg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ESr-N34s5LH",
        "outputId": "bd30a0a0-7f72-4995-f9d8-de237d193232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 300 traces. Baseline avg latency: 55.26 ms\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Simulate ML-KEM Key Generation Traces\n",
        "def simulate_key_gen(n, q, sigma, security_level):\n",
        "    n = max(2, n)  # Avoid log2(1) or less\n",
        "    ntt_time = n * np.log2(n) * 0.01  # ms, arbitrary scaling\n",
        "    sampling_time = sigma * n * 0.005\n",
        "    total_latency = ntt_time + sampling_time\n",
        "    bit_security = min(128 + 64*(security_level-1), np.log2(q) * n / sigma)\n",
        "    trace = np.random.normal(total_latency, sigma, 100)  # 100-dim synthetic trace\n",
        "    return trace, total_latency, bit_security\n",
        "\n",
        "# Generate dataset for each security level\n",
        "def generate_traces(num_traces=100):  # Reduced for faster execution\n",
        "    levels = [1, 3, 5]\n",
        "    params = [(256, 3329, 2.0), (512, 7681, 3.0), (768, 12289, 4.0)]  # NIST-inspired\n",
        "    all_traces, all_latencies, all_securities = [], [], []\n",
        "    for level, (n, q, sigma) in zip(levels, params):\n",
        "        for _ in range(num_traces):\n",
        "            trace, lat, sec = simulate_key_gen(n, q, sigma, level)\n",
        "            all_traces.append(trace)\n",
        "            all_latencies.append(lat)\n",
        "            all_securities.append(sec)\n",
        "    return np.array(all_traces), np.array(all_latencies), np.array(all_securities)\n",
        "\n",
        "traces, latencies, securities = generate_traces()\n",
        "print(f\"Generated {len(traces)} traces. Baseline avg latency: {np.mean(latencies):.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2: Autoencoder for Trace Compression (100D -> 8D latent)"
      ],
      "metadata": {
        "id": "26AsWkI4vOZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Autoencoder for Trace Compression (100D -> 8D latent)\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim=8):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 64), nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        recon = self.decoder(latent)\n",
        "        return recon, latent\n",
        "\n",
        "# Train Autoencoder\n",
        "def train_autoencoder(traces, epochs=5, batch_size=32):  # Reduced epochs for speed\n",
        "    dataset = TensorDataset(torch.tensor(traces, dtype=torch.float32))\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    model = Autoencoder()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for data in loader:\n",
        "            inputs = data[0]\n",
        "            recon, _ = model(inputs)\n",
        "            loss = criterion(recon, inputs)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "ae_model = train_autoencoder(traces)\n",
        "\n",
        "# Precompute latent representations\n",
        "def precompute_latents(ae_model, traces):\n",
        "    ae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        latents = ae_model.encoder(torch.tensor(traces, dtype=torch.float32)).numpy()\n",
        "    return latents\n",
        "\n",
        "latents = precompute_latents(ae_model, traces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuAMzCyptdQW",
        "outputId": "3e32411f-46a1-4668-8a7d-c2d24980ea1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 1933.3258\n",
            "Epoch 2/5, Loss: 1858.8296\n",
            "Epoch 3/5, Loss: 746.4144\n",
            "Epoch 4/5, Loss: 131.7284\n",
            "Epoch 5/5, Loss: 45.1803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Simple REINFORCE Policy Gradient"
      ],
      "metadata": {
        "id": "Z-YoWlhUyenB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Simple REINFORCE Policy Gradient\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.mu = nn.Linear(32, action_dim)\n",
        "        self.log_std = nn.Linear(32, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mu = self.mu(x)\n",
        "        log_std = self.log_std(x)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "# Parameters\n",
        "state_dim = 8\n",
        "action_dim = 3\n",
        "lr = 0.0003\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "min_action = torch.tensor([128., 2000., 1.0])\n",
        "max_action = torch.tensor([1024., 16384., 5.0])\n",
        "\n",
        "# Initialize policy\n",
        "policy = Policy(state_dim, action_dim)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "baseline_lat = np.mean(latencies)\n",
        "baseline_sec = np.mean(securities)\n",
        "\n",
        "last_reward = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    states = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    for _ in range(batch_size):\n",
        "        state_idx = np.random.randint(0, len(latents))\n",
        "        state = torch.tensor(latents[state_idx], dtype=torch.float32)\n",
        "        mu, std = policy(state)\n",
        "        dist = Normal(mu, std)\n",
        "        action = dist.sample()\n",
        "        action = torch.clamp(action, min_action, max_action)\n",
        "        log_prob = dist.log_prob(action).sum()\n",
        "\n",
        "        n, q, sigma = action.numpy()\n",
        "        _, lat, sec = simulate_key_gen(n, q, sigma, 3)\n",
        "        reward = -lat / baseline_lat\n",
        "        if sec < baseline_sec * 0.95:\n",
        "            reward -= 100\n",
        "\n",
        "        states.append(state)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Normalize rewards\n",
        "    rewards_np = np.array(rewards)\n",
        "    last_reward = np.mean(rewards_np)\n",
        "    rewards_norm = (rewards_np - np.mean(rewards_np)) / (np.std(rewards_np) + 1e-8)\n",
        "    rewards = torch.tensor(rewards_norm, dtype=torch.float32)\n",
        "\n",
        "    # Loss\n",
        "    log_probs = torch.stack(log_probs)\n",
        "    loss = - (log_probs * rewards).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Avg Reward: {last_reward:.4f}\")\n",
        "\n",
        "# Quantize policy and AE\n",
        "policy.eval()\n",
        "quantized_policy = quantization.quantize_dynamic(policy, {nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_policy.state_dict(), 'policy_quantized.pth')\n",
        "policy_size = os.path.getsize('policy_quantized.pth') / 1024\n",
        "\n",
        "ae_model.eval()\n",
        "quantized_ae = quantization.quantize_dynamic(ae_model, {nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_ae.state_dict(), 'ae_quantized.pth')\n",
        "ae_size = os.path.getsize('ae_quantized.pth') / 1024\n",
        "\n",
        "print(f\"Quantized Policy size: {policy_size:.2f} KB, AE {ae_size:.2f} KB (Total: {policy_size + ae_size:.2f} KB)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbsMFdB4tDL2",
        "outputId": "269bb668-e874-45f7-d5d6-3ac6f9f51ace"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: -962605350912.0000, Avg Reward: -0.1956\n",
            "Epoch 2/10, Loss: 22719364071424.0000, Avg Reward: -0.1969\n",
            "Epoch 3/10, Loss: 611376300032.0000, Avg Reward: -0.1985\n",
            "Epoch 4/10, Loss: 125128622080.0000, Avg Reward: -0.1897\n",
            "Epoch 5/10, Loss: -31119454208.0000, Avg Reward: -0.1940\n",
            "Epoch 6/10, Loss: -33906855936.0000, Avg Reward: -0.1969\n",
            "Epoch 7/10, Loss: -16082655232.0000, Avg Reward: -0.1897\n",
            "Epoch 8/10, Loss: 1631894272.0000, Avg Reward: -0.1966\n",
            "Epoch 9/10, Loss: -2859771904.0000, Avg Reward: -0.2027\n",
            "Epoch 10/10, Loss: -2904429568.0000, Avg Reward: -0.1898\n",
            "Quantized Policy size: 9.18 KB, AE 26.80 KB (Total: 35.98 KB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Evaluate and Log Metrics"
      ],
      "metadata": {
        "id": "4Y8DLvRgyvAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(policy, num_episodes=100):\n",
        "\n",
        "    opt_latencies, opt_securities = [], []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state_idx = np.random.randint(0, len(latents))\n",
        "        state = torch.tensor(latents[state_idx], dtype=torch.float32)\n",
        "        mu, std = policy(state)\n",
        "        action = mu  # Use mean for evaluation\n",
        "        action = torch.clamp(action, min_action, max_action)\n",
        "        n, q, sigma = action.detach().numpy()\n",
        "        _, lat, sec = simulate_key_gen(n, q, sigma, 3)\n",
        "        opt_latencies.append(lat)\n",
        "        opt_securities.append(sec)\n",
        "\n",
        "    baseline_lat = np.mean(latencies)\n",
        "    opt_lat = np.mean(opt_latencies)\n",
        "    reduction = (baseline_lat - opt_lat) / baseline_lat * 100\n",
        "    avg_sec = np.mean(opt_securities)\n",
        "\n",
        "    metrics = {\n",
        "        'Baseline Latency (ms)': baseline_lat,\n",
        "        'Optimized Latency (ms)': opt_lat,\n",
        "        'Latency Reduction (%)': reduction,\n",
        "        'Avg Security Score (bits)': avg_sec,\n",
        "        'Model Size (KB)': policy_size + ae_size\n",
        "    }\n",
        "    print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    # Log to CSV\n",
        "    df = pd.DataFrame([metrics])\n",
        "    df['Last Reward'] = [last_reward]  # Last mean reward\n",
        "    df.to_csv('metrics.csv', index=False)\n",
        "    print(\"Metrics logged to metrics.csv\")\n",
        "\n",
        "evaluate(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r8ep0DHs_bZ",
        "outputId": "73998a45-fc5a-449a-813c-9ee013472297"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics: {'Baseline Latency (ms)': np.float64(55.25750400184616), 'Optimized Latency (ms)': np.float32(9.599998), 'Latency Reduction (%)': np.float64(82.62679676266167), 'Avg Security Score (bits)': np.float64(256.0), 'Model Size (KB)': 35.978515625}\n",
            "Metrics logged to metrics.csv\n"
          ]
        }
      ]
    }
  ]
}